{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "FinalExam_Lab.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aidarvaleev1998/AML-DS-2021/blob/main/FinalExam_Lab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "average-extension"
      },
      "source": [
        "## <center>Final Exam Lab\n",
        "```\n",
        "- Advanced Machine Learning, Innopolis University \n",
        "- Professor: Muhammad Fahim \n",
        "- Teaching Assistant: Gcinizwe Dlamini\n",
        "```\n",
        "<hr>\n",
        "\n",
        "```\n",
        "Tasks:\n",
        "  1. Data Preprocessing (5 points)\n",
        "  2. Conditional Generative adversarial network definition (5 points)\n",
        "  3. Conditional Generative adversarial network training (10 points)\n",
        "  4. Text explainer implemetation using Lime or Shap (5 bonus points)\n",
        "```\n",
        "\n",
        "<hr>"
      ],
      "id": "average-extension"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "composite-convenience"
      },
      "source": [
        "## The Dataset\n",
        "\n",
        "For this task the 20 newsgroups text dataset is used. [LINK](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html)"
      ],
      "id": "composite-convenience"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "average-creek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3546ee2-445e-4172-9bee-a10b8619ed57"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "\n",
        "import nltk, string, re\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Available device : {device}\")"
      ],
      "id": "average-creek",
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Available device : cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rubber-november"
      },
      "source": [
        "## Task 1: Preprocessing of Dataset (5 points)\n",
        "\n",
        "\n",
        "\n",
        "1.  Loading and cleaning of Text data:\n",
        "    * Choose 4 categories from the dataset  \n",
        "    * Implement a method `clean_text` which will take text then make text lowercase, remove punctuation, whitespaces and stopwords\n",
        "    * Plot the distribution of classes/categories"
      ],
      "id": "rubber-november"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "secret-publisher"
      },
      "source": [
        "categories = ['alt.atheism', 'comp.graphics', 'rec.autos', 'sci.space'] #TODO: Choose 4 categories from the dataset\n",
        "newsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "newsgroups_valid = fetch_20newsgroups(subset='test', categories=categories, remove=('headers', 'footers', 'quotes'))\n"
      ],
      "id": "secret-publisher",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "forward-invitation"
      },
      "source": [
        "def clean_text(text):\n",
        "    \"\"\" Function to perform common NLP pre-processing tasks. \"\"\"\n",
        "    # make lowercase\n",
        "    text = text.lower()\n",
        "    # remove punctuation\n",
        "    text = word_tokenize(text)\n",
        "    # remove numbers\n",
        "    text = [re.sub(r\"\\d*\", \"\", w) for w in text]\n",
        "    # remove whitespaces\n",
        "    text = [re.sub(r\" *\", \"\", w) for w in text]\n",
        "    # remove stopwords\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    text = [w for w in text if w not in stop_words]\n",
        "    # remove short words\n",
        "    text = [w for w in text if len(w) > 3]\n",
        "    return \" \".join(text)"
      ],
      "id": "forward-invitation",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "generic-cathedral"
      },
      "source": [
        "train_sentences = []\n",
        "validation_sentences = []\n",
        "\n",
        "train_labels = []\n",
        "validation_labels = []\n",
        "\n",
        "\n",
        "# Clean training sentences\n",
        "for id in range(len(newsgroups_train.data)):\n",
        "    text = clean_text(newsgroups_train.data[id])\n",
        "    label = newsgroups_train.target[id]\n",
        "    if text:\n",
        "        train_sentences.append(text)\n",
        "        train_labels.append(label)\n",
        "\n",
        "# Clean validation sentences\n",
        "for id in range(len(newsgroups_valid.data)):\n",
        "    text = clean_text(newsgroups_valid.data[id])\n",
        "    label = newsgroups_valid.target[id]\n",
        "    if text:\n",
        "        validation_sentences.append(text)\n",
        "        validation_labels.append(label)"
      ],
      "id": "generic-cathedral",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "careful-anniversary"
      },
      "source": [
        "## Create vocabulary"
      ],
      "id": "careful-anniversary"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qualified-excellence"
      },
      "source": [
        "# Create tokenizer\n",
        "en_tokenizer = get_tokenizer('spacy', language='en')\n",
        "\n",
        "# Create vocabulary\n",
        "def build_vocab(sentences, tokenizer):\n",
        "    counter = Counter()\n",
        "    for sentence in sentences:\n",
        "        counter.update(tokenizer(sentence))\n",
        "    return Vocab(counter, specials=['<unk>', '<pad>'])\n",
        "\n",
        "vocabulary = build_vocab(train_sentences, en_tokenizer)"
      ],
      "id": "qualified-excellence",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tc2W3qQxC9IH"
      },
      "source": [
        "VOCAB_SIZE = len(vocabulary) + 2"
      ],
      "id": "Tc2W3qQxC9IH",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unlike-filename"
      },
      "source": [
        "## Add padding "
      ],
      "id": "unlike-filename"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecological-hormone"
      },
      "source": [
        "max_len = 128\n",
        "\n",
        "# Add Padding \n",
        "def create_dataset(sentences, labels, en_tokenizer, vocab, max_len=128):\n",
        "    res = []\n",
        "    for sentence in sentences:\n",
        "        sentence_tokens = [vocab[token] for token in en_tokenizer(sentence)]\n",
        "        if len(sentence_tokens) <= max_len:\n",
        "            sentence_tokens = sentence_tokens + [vocab['<pad>']]*(max_len-len(sentence_tokens))\n",
        "        else:\n",
        "            sentence_tokens = sentence_tokens[:max_len]\n",
        "        sentence_tensor = torch.tensor(sentence_tokens,dtype=torch.long)\n",
        "        res.append(sentence_tensor)\n",
        "        \n",
        "    return TensorDataset(torch.stack(res),torch.from_numpy(np.array(labels)))\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "PAD_IDX = vocabulary['<pad>']\n",
        "\n",
        "train_dataset = create_dataset(train_sentences,train_labels, en_tokenizer, vocabulary)\n",
        "validation_dataset = create_dataset(validation_sentences, validation_labels, en_tokenizer, vocabulary)"
      ],
      "id": "ecological-hormone",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y3ntu_dMbUn8"
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validation_loader = DataLoader(validation_dattrain_dsaset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "id": "y3ntu_dMbUn8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "attached-reservoir"
      },
      "source": [
        "## Task 2: Conditional Generative adversarial network definition (5 points)\n",
        "\n",
        "1.  Models Definition:\n",
        "    * Define the Generator & Discriminator network (Achitecture of your choice) "
      ],
      "id": "attached-reservoir"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "id": "otherwise-reference",
        "outputId": "82e8ab78-1fea-44ff-dba2-f1ed06626a57"
      },
      "source": [
        "# TODO: Implement the Generator & Discriminator class\n",
        "class Generator(nn.Module):\n",
        "    # initializersremove whitespaces\n",
        "    def __init__(self, emb_dim, hidden_dim): # noise_dim = emb_dim\n",
        "        super(Generator, self).__init__()\n",
        "        self.embeddings = nn.Embedding(VOCAB_SIZE, emb_dim, padding_idx=PAD_IDX)\n",
        "        self.lstm = nn.LSTM(emb_dim, hidden_dim, batch_first=True)\n",
        "        self.lstm2out = nn.Linear(hidden_dim, VOCAB_SIZE)\n",
        "        self.softmax = nn.LogSoftmax(dim=-1)\n",
        "        self.cond_emb = nn.Embedding(4, emb_dim)\n",
        "\n",
        "    # forward method. Condition is should be incorporated to the model input \n",
        "    def forward(self, x, c, hidden, need_hidden=False):\n",
        "        emb = self.emb(x)  # batch_size * len * emb_dim\n",
        "        c = self.cond_emb(c).reshape((c.shape[0], 1, -1))\n",
        "        emb = torch.cat([c, emb], 1)\n",
        "        if len(x.size()) == 1:\n",
        "            emb = emb.unsqueeze(1)  # batch_size * 1 * emb_dim\n",
        "\n",
        "        out, hidden = self.lstm(out, hidden)  # out: batch_size * seq_len * hidden_dim\n",
        "        out = out.contiguous().view(-1, self.hidden_dim)  # out: (batch_size * len) * hidden_dim\n",
        "        out = self.lstm2out(out)  # (batch_size * seq_len) * vocab_size\n",
        "        pred = self.softmax(out)\n",
        "        \n",
        "        if need_hidden:\n",
        "            return pred, hidden\n",
        "        else:\n",
        "            return pred\n",
        "\n",
        "    def sample(self, c, num_samples, batch_size):\n",
        "        num_batch = num_samples // batch_size + 1 if num_samples != batch_size else 1\n",
        "        samples = torch.zeros(num_batch * batch_size, max_len).long()\n",
        "\n",
        "        # Generate sentences with multinomial sampling strategy\n",
        "        for b in range(num_batch):\n",
        "            hidden = (\n",
        "                torch.zeros(1, batch_size, self.hidden_dim).cuda(),\n",
        "                torch.zeros(1, batch_size, self.hidden_dim).cuda()\n",
        "            )\n",
        "            inp = torch.LongTensor([] * batch_size)\n",
        "            if self.gpu:\n",
        "                inp = inp.cuda()\n",
        "\n",
        "            for i in range(max_len):\n",
        "                out, hidden = self.forward(inp, c, hidden, need_hidden=True)  # out: batch_size * vocab_size\n",
        "                next_token = torch.multinomial(torch.exp(out), 1)  # batch_size * 1 (sampling from each row)\n",
        "                samples[b * batch_size:(b + 1) * batch_size, i] = next_token.view(-1)\n",
        "                inp = next_token.view(-1)\n",
        "        samples = samples[:num_samples]\n",
        "\n",
        "        return samples\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self, emb_dim, hidden_dim, dropout):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        \n",
        "        self.embeddings = nn.Embedding(VOCAB_SIZE, emb_dim, padding_idx=PAD_IDX)\n",
        "        self.cond_emb = nn.Embedding(4, emb_dim)\n",
        "        self.gru = nn.GRU(emb_dim, hidden_dim, num_layers=2, bidirectional=True, dropout=dropout)\n",
        "        self.dense = nn.Linear(2 * 2 * hidden_dim, 1)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    # forward method. Note: Condition is should be incorporated to the model input\n",
        "    def forward(self, x, c):\n",
        "        emb = self.embeddings(x)  # batch_size * seq_len * emb_dim\n",
        "        emb = emb.permute(1, 0, 2)  # seq_len * batch_size * emb_dim\n",
        "\n",
        "        c = self.cond_emb(c).reshape((1, c.shape[0], -1))\n",
        "        out = torch.cat([emb, c], 0)\n",
        "        _, hidden = self.gru(out)  # 4 * batch_size * hidden_dim\n",
        "        hidden = hidden.permute(1, 0, 2).contiguous()  # batch_size * 4 * hidden_dim\n",
        "        out = self.dense(hidden.view(-1, 4 * self.hidden_dim))\n",
        "        out = torch.tanh(out)\n",
        "        return out\n",
        "\n",
        "# define discriminator and generator\n",
        "# TODO: specify the input and output size\n",
        "\n",
        "D = Discriminator(emb_dim=32, hidden_dim=32, dropout=0.3).to(device).float()\n",
        "G = Generator(emb_dim=32, hidden_dim=32).to(device).float()\n",
        "\n",
        "print(G)\n",
        "print()\n",
        "print(D)"
      ],
      "id": "otherwise-reference",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-4f35ab061350>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;31m# TODO: specify the input and output size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDiscriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0mG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memb_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    671\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 673\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m     def register_backward_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    407\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    669\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    670\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 671\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sudden-delta"
      },
      "source": [
        "## Task 3: Conditional Generative adversarial network training (10 points)\n",
        "\n",
        "* Implement the Conditional Generative adversarial network training procedure \n",
        "* Define the optimizers for Generator and Discriminator network\n",
        "* Define the loss functions\n",
        "* Add Tensorboard to log the Generator and Discriminator loss (for both Training and Validation). For discriminator the loss on fake samples and real samples should be logged separately \n",
        "\n",
        "**NOTE:** It is not important that the loss decreases during the training loop for this task. It is important that the training procedure is correctly implemented"
      ],
      "id": "sudden-delta"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "liable-castle",
        "outputId": "ad80941f-028e-4b74-cf13-25d3cb5063a8"
      },
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "%load_ext tensorboard\n",
        "\n",
        "writer = SummaryWriter()\n",
        "\n",
        "# params\n",
        "learning_rate = 0.0001\n",
        "n_epochs = 10\n",
        "# TODO: Create optimizers for the discriminator and generator\n",
        "d_optimizer = optim.Adam(D.parameters(), learning_rate)\n",
        "g_optimizer = optim.SGD(G.parameters(), learning_rate)\n",
        "\n",
        "# fixed noise for validation \n",
        "fixed_noise = torch.normal(0,1, (len(validation_dataset), 32), dtype=torch.float, device=device)"
      ],
      "id": "liable-castle",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-a98fefb569bc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# fixed noise for validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mfixed_noise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "promotional-extra",
        "outputId": "6f02d45e-24a7-4c7b-d55b-fb2083745eff"
      },
      "source": [
        "## TODO: Implement the training procedure and log train & validation loss using tensorboard\n",
        "loss_function = nn.BCELoss()\n",
        "for epoch in range(n_epochs):\n",
        "    G.train()\n",
        "    D.train()\n",
        "    for x,y in train_loader:\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # TRAIN THE DISCRIMINATOR\n",
        "        # Step 1: Zero gradients (zero_grad)\n",
        "        # Step 2: Train with real\n",
        "        # Step 3: Compute the discriminator losses on real \n",
        "\n",
        "        d_optimizer.zero_grad()\n",
        "        D_real = D(x, y)\n",
        "        d_real_loss = loss_function(D_real, torch.ones(D_real.shape[0], 1).to(device))\n",
        "\n",
        "        # Step 4: Train with fake\n",
        "        # Step 5: Generate fake and move x to GPU, if available\n",
        "        # Step 6: Compute the discriminator losses on fake \n",
        "        # Step 7: add up loss and perform backprop\n",
        "        \n",
        "        fake = G.sample(y, y.shape[0], y.shape[0])\n",
        "        \n",
        "        # Compute the discriminator losses on fake \n",
        "        d_optimizer.zero_grad()           \n",
        "        D_fake = D(fake)\n",
        "        d_fake_loss = loss_function(D_fake, torch.zeros(D_real.shape[0], 1).to(device))\n",
        "        \n",
        "        # add up loss and perform backprop\n",
        "        d_loss = d_real_loss + d_fake_loss\n",
        "        d_loss.backward()\n",
        "        d_optimizer.step()\n",
        "        \n",
        "        \n",
        "        #TRAIN THE GENERATOR (Train with fake and flipped labels)\n",
        "        g_optimizer.zero_grad()\n",
        "        \n",
        "        # Step 1: Zero gradients  Generator\n",
        "        # Step 2: Generate fake from random noise (z)\n",
        "        # Step 3: Compute the discriminator losses on fake using flipped labels!\n",
        "        # Step 4: Perform backprop and take optimizer step\n",
        "        fake = G.sample(y, y.shape[0], y.shape[0])\n",
        "        D_g = D(fake, y)\n",
        "        g_loss = loss_function(D_g, torch.ones(D_g.shape[0], 1).to(device))\n",
        "        g_loss.backward()\n",
        "        g_optimizer.step()\n",
        "\n",
        "    # validation\n",
        "    # with torch.no_grad():\n",
        "    #     D.eval()\n",
        "    #     G.eval()\n",
        "    #     for x, y in validation_loader:\n",
        "    #         x = x.to(device)\n",
        "    #         y = y.to(device)\n",
        "\n",
        "    writer.add_scalar(\"Train Loss D Real\", d_real_loss, epoch)\n",
        "    writer.add_scalar(\"Train Loss D Fake\", d_fake_loss, epoch)\n",
        "    writer.add_scalar(\"Train Loss G\", g_loss, epoch)"
      ],
      "id": "promotional-extra",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-b9f261c086e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eastern-guest"
      },
      "source": [
        "## Launch Tensorboard"
      ],
      "id": "eastern-guest"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oriented-examination"
      },
      "source": [
        "%tensorboard --logdir ./runs"
      ],
      "id": "oriented-examination",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "developmental-sewing"
      },
      "source": [
        "## Task 4: (Optional): Text explainer implemetation using Lime or Shap (5 bonus points)\n",
        "\n",
        "Using the [20 newsgroups](https://scikit-learn.org/0.19/datasets/twenty_newsgroups.html) text dataset is used.\n",
        "Create a simple(i.e Decision tree, Random Forest) multi-class classifier and explain the classifiers predictions with the help of LIME or SHARP. \n",
        "\n",
        "**Note:** Use TF-IDF for feature extraction"
      ],
      "id": "developmental-sewing"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dressed-mayor"
      },
      "source": [
        "newsgroups_train = fetch_20newsgroups(subset='train')\n",
        "newsgroups_test = fetch_20newsgroups(subset='test')\n",
        "\n",
        "clf = None"
      ],
      "id": "dressed-mayor",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "detailed-providence"
      },
      "source": [
        "## <center>Solution should be pushed to github and link to github submitted to Moodle</center>"
      ],
      "id": "detailed-providence"
    }
  ]
}